{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Setting Wandb project credentials\n",
        "'''\n",
        "entity=\"cs21m007_cs21m013\"\n",
        "project=\"checking\""
      ],
      "metadata": {
        "id": "6uPI7bXhysve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ostY-LSFR3Vl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Class: Layer\n",
        "Definations: Initialize_params, activation_fn\n",
        "'''\n",
        "\n",
        "import numpy as np  # numpy for implementing array operations inside the functions.\n",
        "class Layer:\n",
        "    '''\n",
        "    Method: __init__ constructor for base initializations\n",
        "    Input: no of hidden units of the layer, activation for the layer\n",
        "    Output: None\n",
        "    '''\n",
        "    def __init__(self, hidden_units: int, activation:str=None):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation = activation\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "    '''\n",
        "    Method: intialize_params for initializing the weights and biases of each layer\n",
        "    Input: dimension of the input to the layer, no of hidden neurons in the layer, the initialization type(Random or Xavier)\n",
        "    Output: None\n",
        "    ''' \n",
        "    def initialize_params(self, n_in, hidden_units,init_type):\n",
        "        np.random.seed(2)\n",
        "        if init_type==\"Random\":\n",
        "            self.W = 0.01*np.random.randn(n_in, hidden_units)\n",
        "            self.b = 0.01*np.random.randn(1,hidden_units)\n",
        "\n",
        "        elif init_type==\"Xavier\":\n",
        "            self.W = np.random.randn(n_in, hidden_units) * np.sqrt(2/n_in) \n",
        "            self.b = np.zeros((1, hidden_units))\n",
        "\n",
        "    '''\n",
        "    Method: activation_fn for defining the activation functions and thier derivatives to be used by the layers\n",
        "    Input: The computed pre activation of each layer\n",
        "    Output: calculates the activation value for forward prop or the derivative of the activation for backward prop\n",
        "    '''\n",
        "    def activation_fn(self, z, derivative=False):\n",
        "        '''\n",
        "        Relu activation and its derivative\n",
        "        '''\n",
        "        if self.activation == 'relu':\n",
        "            if derivative:\n",
        "                return np.where(z<=0,0,1)\n",
        "            return np.maximum(0, z)\n",
        "        '''\n",
        "        sigmoid activation and its derivative\n",
        "        '''\n",
        "        if self.activation == 'sigmoid':\n",
        "            if derivative:\n",
        "                return (1 / (1 + np.exp(-z))) * (1-(1 / (1 + np.exp(-z))))\n",
        "            return (1 / (1 + np.exp(-z)))\n",
        "        '''\n",
        "        tanh activation and its derivative\n",
        "        '''\n",
        "        if self.activation == 'tanh':\n",
        "            t=(np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
        "            if derivative:\n",
        "                return (1-t**2)\n",
        "            return t\n",
        "        '''\n",
        "        softmax function and its derivativ for the output layer with 10 neurons.\n",
        "        '''\n",
        "        if self.activation == 'softmax':\n",
        "            if derivative: \n",
        "                exp = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "                return exp / np.sum(exp, axis=0) * (1 - exp / np.sum(exp, axis=0))\n",
        "            exp = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "            return exp / np.sum(exp, axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8IM9gQN_HzP"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Class: Helper\n",
        "Definations: Accruacy function, compute_loss function, create batches function.\n",
        "'''\n",
        "import numpy as np  # numpy for performing array operations.\n",
        "class Helper:\n",
        "    '''\n",
        "    Method: accuracy\n",
        "    Input: the true labels and the predicted proababilities generated by the model.\n",
        "    Output: Returns the accuracy of the model on the data.\n",
        "    '''\n",
        "    def accuracy(self,y,y_hat):\n",
        "        c = np.argmax(y_hat, axis=1) == np.argmax(y, axis=1)\n",
        "        acc = list(c).count(True) / len(c) * 100\n",
        "        return acc\n",
        "\n",
        "    '''\n",
        "    Method: compute_loss\n",
        "    Input: the true label, predicted probabilities, loss_type(SquarredError or CrossEntropy) and the regularization coefficient if any.\n",
        "    Output: Returns the value of the loss (SE or CE) plus the regularization loss if any.\n",
        "    '''\n",
        "    def compute_loss(self,Y, Y_hat,layers,loss_type=\"CrossEntropy\",reg=0):\n",
        "        if loss_type==\"CrossEntropy\":\n",
        "            m = Y.shape[0]\n",
        "            L = -1./m * np.sum(Y * np.log(Y_hat+0.0000000001))\n",
        "        elif loss_type==\"SquaredError\":\n",
        "            L = np.mean((Y- Y_hat)**2)\n",
        "\n",
        "        if reg!=0:\n",
        "            reg_error = 0.0                                                                       \n",
        "            for idx in layers.keys() :\n",
        "              reg_error += (reg/2)*(np.sum(np.square(layers[idx].W))) \n",
        "            L = L + reg_error\n",
        "\n",
        "        return L\n",
        "    \n",
        "    '''\n",
        "    Method: create_batches depending on the batch size for training\n",
        "    Input: the training data (X,y) and the batch size\n",
        "    Ouput: Batches of data for training based on the batch size.\n",
        "    '''\n",
        "    def create_batches(self,x, y, batch_size):\n",
        "        m = x.shape[0]\n",
        "        num_batches = m / batch_size\n",
        "        batches = []\n",
        "        for i in range(int(num_batches+1)):\n",
        "            batch_x = x[i*batch_size:(i+1)*batch_size]\n",
        "            batch_y = y[i*batch_size:(i+1)*batch_size]\n",
        "            batches.append((batch_x, batch_y))\n",
        "        \n",
        "        if m % batch_size == 0:\n",
        "            batches.pop(-1)\n",
        "\n",
        "        return batches\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgJzfc3JR97s"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Clss: Neural_Network\n",
        "Definitions: Constructor __init__, add , forward, backward, GDoptimize, \n",
        "              SGDMoptimize, Nesterovoptimize, RMSpropoptimize, Adamoptimize, \n",
        "              Nadamoptimize, fit, predict\n",
        "'''\n",
        "import numpy as np  # numpy to tackle all array related operations\n",
        "from sklearn.model_selection import train_test_split  # train test split for splitting the train data into further train and validation.\n",
        "class Neural_Network:\n",
        "    '''\n",
        "    Method: __init__ constructor for base initialization of layers, cache and gradients for each layer.\n",
        "    Input: None\n",
        "    Output: None\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.layers = dict()\n",
        "        self.cache = dict()\n",
        "        self.grads = dict()\n",
        "\n",
        "    '''\n",
        "    Method: add, to add the layer objects to the model (object of neural network).\n",
        "    Input: layer dictionary\n",
        "    Output: None\n",
        "    '''    \n",
        "    def add(self, layer):\n",
        "        self.layers[len(self.layers)+1] = layer\n",
        "\n",
        "    '''\n",
        "    Method: forward, for forward propagation of the model.\n",
        "    Input: input data and the initilization type of the W,b's of the layer\n",
        "    Output: Returns the predicted probability distribution after forward propagation\n",
        "    '''\n",
        "    def forward(self, x, init_type=\"Xavier\"):\n",
        "        for idx, layer in self.layers.items():\n",
        "\n",
        "            layer.input = np.array(x, copy=True)\n",
        "            if layer.W is None:\n",
        "                layer.initialize_params(layer.input.shape[-1], layer.hidden_units,init_type)  # initilaize the weights and the biases.\n",
        "\n",
        "            layer.Z = x @ layer.W + layer.b # linear pre activation\n",
        "        \n",
        "            if layer.activation is not None:\n",
        "                layer.A = layer.activation_fn(layer.Z) #applying non-linear activation function\n",
        "                x = layer.A\n",
        "            else:\n",
        "                x = layer.Z\n",
        "            self.cache[f'W{idx}'] = layer.W # storing the weights of the layer\n",
        "            self.cache[f'Z{idx}'] = layer.Z # storing the pre activation values of each layer\n",
        "            self.cache[f'A{idx}'] = layer.A # storing the activation values of each layer.\n",
        "        return x\n",
        "\n",
        "    '''\n",
        "    Method: backward, for backward propagation for generating the gradients for weight updation.\n",
        "    Input: true labels, loss_type, regularization coefficient\n",
        "    Output: None, but save the gradients in the grad dictionary of the model(Neural Network object)\n",
        "    '''\n",
        "    def backward(self, y, loss_type,reg=0):\n",
        "        last_layer_idx = max(self.layers.keys())\n",
        "        m = y.shape[0]\n",
        "        for idx in reversed(range(1, last_layer_idx+1)):  # move from output to inputs\n",
        "            if idx == last_layer_idx:\n",
        "                if loss_type==\"CrossEntropy\":\n",
        "                    self.grads[f'dZ{idx}'] = self.cache[f'A{idx}'] - y  # gradient wrt output layer for cross entropy loss\n",
        "                elif loss_type==\"SquaredError\":\n",
        "                    self.grads[f'dZ{idx}'] = (self.cache[f'A{idx}'] - y) * self.layers[idx].activation_fn(self.cache[f'Z{idx}'], derivative=True) # gradients wrt output layer for squared error loss\n",
        "            else:\n",
        "                self.grads[f'dZ{idx}'] = self.grads[f'dZ{idx+1}'] @ self.cache[f'W{idx+1}'].T *\\\n",
        "                                        self.layers[idx].activation_fn(self.cache[f'Z{idx}'], derivative=True) # gradients directly wrt to the pre-activation for each layer.\n",
        "\n",
        "\n",
        "            self.grads[f'dW{idx}'] = 1 / m * self.layers[idx].input.T @ self.grads[f'dZ{idx}'] + reg*self.layers[idx].W # gradients wrt the weights of each layer\n",
        "            self.grads[f'db{idx}'] = 1 / m * np.sum(self.grads[f'dZ{idx}'], axis=0, keepdims=True)  # gradients wrt the biases of each layer.\n",
        "            \n",
        "            assert self.grads[f'dW{idx}'].shape == self.cache[f'W{idx}'].shape\n",
        "\n",
        "    '''\n",
        "    Method: GDoptimize, basically the vanilla gradient descent\n",
        "    Input: learning_rate, idx indicating the layer index\n",
        "    Output: None, but performs the weight updations wrt the gradients.\n",
        "    '''\n",
        "    def GDoptimize(self, idx, learning_rate=1e-3):\n",
        "        \n",
        "        self.layers[idx].W -= learning_rate * self.grads[f'dW{idx}']  # W update\n",
        "        self.layers[idx].b -= learning_rate * self.grads[f'db{idx}']  # b update\n",
        "\n",
        "    '''\n",
        "    Method: SGDMoptimize, basically the momemtum based gradient descent.\n",
        "    Input: learning_rate, idx, mu - fixed momentum coefficient\n",
        "    Output: None, but weights, biases are updated\n",
        "    '''\n",
        "    def SGDMoptimize(self, idx, learning_rate=1e-3, mu=0.99):\n",
        "        m = dict()\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "\n",
        "        m[f'W{idx}'] = m[f'W{idx}'] * mu - learning_rate * self.grads[f'dW{idx}'] # momentum wrt W\n",
        "        m[f'b{idx}'] = m[f'b{idx}'] * mu - learning_rate * self.grads[f'db{idx}'] # momentum wrt b\n",
        "\n",
        "        self.layers[idx].W += m[f'W{idx}']  # W update\n",
        "        self.layers[idx].b += m[f'b{idx}']  # b update\n",
        "\n",
        "    '''\n",
        "    Method: Nesterovoptimize, nesterov accelerated gradien descent.\n",
        "    Input: learning rate, mu - fixed momentum coefficient, idx of the layer\n",
        "    Output: None, but updates the parameters(W,b)\n",
        "    '''\n",
        "    def Nesterovoptimize(self, idx, learning_rate=1e-3, mu=0.99):\n",
        "        m = dict()\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "\n",
        "        mW_prev =  np.array(m[f'W{idx}'], copy=True)\n",
        "        mb_prev = np.array(m[f'b{idx}'], copy=True)\n",
        "\n",
        "        m[f'W{idx}'] = m[f'W{idx}'] * mu - learning_rate * self.grads[f'dW{idx}'] # moemtum update wrt W\n",
        "        m[f'b{idx}'] = m[f'b{idx}'] * mu - learning_rate * self.grads[f'db{idx}'] # momentum update wrt b\n",
        "        # using the lookaheads\n",
        "        w_update = -mu * mW_prev + (1 + mu) * m[f'W{idx}'] \n",
        "        b_update = -mu * mb_prev + (1 + mu) * m[f'b{idx}']\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  # b update\n",
        "\n",
        "    '''\n",
        "    Method: RMSpropoptimize, basicall RMSprop gradient descent.\n",
        "    Input: idx of layer, learning rate, decay rate and epsilon\n",
        "    Output: None, updates the parameters.\n",
        "    '''\n",
        "    def RMSpropoptimize(self, idx, learning_rate=1e-3,decay_rate=0.99, epsilon=1e-8):\n",
        "        v = dict()\n",
        "        for i in self.layers.keys():\n",
        "            v[f'W{i}'] = 0\n",
        "            v[f'b{i}'] = 0\n",
        "        # using the learning rate decay\n",
        "        v[f'W{idx}'] = decay_rate * v[f'W{idx}'] + (1 - decay_rate) * self.grads[f'dW{idx}'] **2 \n",
        "        v[f'b{idx}'] = decay_rate * v[f'b{idx}'] + (1 - decay_rate) * self.grads[f'db{idx}'] **2\n",
        "        # update values calculation    \n",
        "        w_update = -learning_rate * self.grads[f'dW{idx}'] / (np.sqrt(v[f'W{idx}'] + epsilon))\n",
        "        b_update = -learning_rate * self.grads[f'db{idx}'] / (np.sqrt(v[f'b{idx}']+ epsilon))\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  # b update\n",
        "\n",
        "    '''\n",
        "    Method: Adamoptimize, Adam optimizer\n",
        "    Input: idx,steps,learing rate, beta1, beta2 and epsilon\n",
        "    Ouput: None, but updates the parameters\n",
        "    '''\n",
        "    def Adamoptimize(self, idx, steps, learning_rate=1e-3, beta1=0.99, beta2=0.999, epsilon=1e-8): \n",
        "        m = dict()\n",
        "        v = dict()\n",
        "\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "            v[f'W{i}'] = 0\n",
        "            v[f'b{i}'] = 0\n",
        "\n",
        "        dW = self.grads[f'dW{idx}']\n",
        "        db = self.grads[f'db{idx}']\n",
        "\n",
        "        # weights\n",
        "        m[f'W{idx}'] = beta1 * m[f'W{idx}'] + (1 - beta1) * dW\n",
        "        v[f'W{idx}'] = beta2 * v[f'W{idx}'] + (1 - beta2) * dW ** 2 \n",
        "        \n",
        "        # biases\n",
        "        m[f'b{idx}'] = beta1 * m[f'b{idx}'] + (1 - beta1) * db\n",
        "        v[f'b{idx}'] = beta2 * v[f'b{idx}'] + (1 - beta2) * db ** 2 \n",
        "\n",
        "        # take timestep into account for bias correction\n",
        "        mt_w  = m[f'W{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_w = v[f'W{idx}'] / (1 - beta2 ** steps)\n",
        "\n",
        "        mt_b  = m[f'b{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_b = v[f'b{idx}'] / (1 - beta2 ** steps)\n",
        "\n",
        "        w_update = - learning_rate * mt_w / (np.sqrt(vt_w) + epsilon)\n",
        "        b_update = - learning_rate * mt_b / (np.sqrt(vt_b) + epsilon)\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  # b update\n",
        "\n",
        "    '''\n",
        "    Method: Nadamoptimize, nesterov accelerated Adam\n",
        "    Input: idx of layer, steps, learing rate, beat1, beta2, epsilon\n",
        "    Output: None, but updates the parameters.\n",
        "    '''\n",
        "    def Nadamoptimize(self, idx, steps,learning_rate=1e-3, beta1=0.99, beta2=0.999, epsilon=1e-8): \n",
        "        m = dict()\n",
        "        v = dict()\n",
        "\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "            v[f'W{i}'] = 0\n",
        "            v[f'b{i}'] = 0\n",
        "        dW = self.grads[f'dW{idx}']\n",
        "        db = self.grads[f'db{idx}']\n",
        "        # weights\n",
        "        m[f'W{idx}'] = beta1 * m[f'W{idx}'] + (1 - beta1) * dW\n",
        "        v[f'W{idx}'] = beta2 * v[f'W{idx}'] + (1 - beta2) * dW ** 2 \n",
        "            \n",
        "        # biases\n",
        "        m[f'b{idx}'] = beta1 * m[f'b{idx}'] + (1 - beta1) * db\n",
        "        v[f'b{idx}'] = beta2 * v[f'b{idx}'] + (1 - beta2) * db ** 2 \n",
        "\n",
        "        # take timestep into account for bias correction\n",
        "        mt_w  = m[f'W{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_w = v[f'W{idx}'] / (1 - beta2 ** steps)\n",
        "\n",
        "        mt_b  = m[f'b{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_b = v[f'b{idx}'] / (1 - beta2 ** steps)\n",
        "        # accelerated momentum incorporation into adam\n",
        "        w_update = - learning_rate / (np.sqrt(vt_w) + epsilon) * (beta1 * mt_w + (1 - beta1) *  dW / (1 - beta1 ** steps))\n",
        "        b_update = - learning_rate / (np.sqrt(vt_b) + epsilon) * (beta1 * mt_b + (1 - beta1) *  db / (1 - beta1 ** steps))\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  #b update\n",
        "            \n",
        "    '''\n",
        "    Method: fit, used to train the model by combining forward_prop, back_prop and gradient descent weight updation.\n",
        "    Input: Training data, batch_size, epochs, learning rate, optimizer to use, val_split factor, initialization type of the weights and biases, loss type, and the regularization coefficient\n",
        "    Output: None, but performs the training of the model \n",
        "    '''\n",
        "    def fit(self, x_train, y_train,batch_size=32,epochs=500, learning_rate=1e-3, optimizer=\"GD\",val_split=0.1,init_type=\"Xavier\",loss_type=\"CrossEntropy\",reg=0):\n",
        "        train_accs = [] #stores the training accuracy for each epoch\n",
        "        val_accs = [] #stores the validation accuracy after each epoch\n",
        "        help=Helper() #creating a object of the Helper class for helper functions\n",
        "        \n",
        "        '''Initializations'''\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.init_type=init_type\n",
        "        self.reg=reg\n",
        "        self.loss_type=loss_type\n",
        "\n",
        "        '''Splitting the training data into train and val data based on the val_split value''' \n",
        "        x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=val_split,stratify=y_train,random_state=42)\n",
        "\n",
        "        '''Training Cycle'''\n",
        "        for i in range(1, self.epochs+1):\n",
        "            print(f'Epoch {i}')\n",
        "            batches = help.create_batches(x_train, y_train, batch_size) # create batches based on the batch size\n",
        "            epoch_loss = []\n",
        "            steps = 0 #count the steps in each epoch\n",
        "            \n",
        "            for x, y in batches:\n",
        "                steps += 1\n",
        "                '''Forward Propagation'''\n",
        "                preds = self.forward(x,self.init_type)\n",
        "   \n",
        "                '''backward propagation'''\n",
        "                self.backward(y,self.loss_type,self.reg)\n",
        "                \n",
        "                '''update weights and biases of each layer using the corresponding optimizer'''\n",
        "                for idx in self.layers.keys():\n",
        "                    if self.optimizer ==\"GD\":\n",
        "                        self.GDoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"SGDM\":\n",
        "                        self.SGDMoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"Nesterov\":\n",
        "                        self.Nesterovoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"RMSprop\":\n",
        "                        self.RMSpropoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"Adam\":\n",
        "                        self.Adamoptimize(idx, steps, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"Nadam\":\n",
        "                        self.Nadamoptimize(idx, steps, learning_rate=self.learning_rate)\n",
        "                \n",
        "            '''Predict with network on x_train'''\n",
        "            train_preds = self.forward(x_train)\n",
        "            train_loss = help.compute_loss(y, preds,self.layers,self.loss_type,self.reg)\n",
        "            train_acc=help.accuracy(train_preds,y_train)\n",
        "            train_accs.append(train_acc)\n",
        "            \n",
        "            '''predcit with network on validation data'''\n",
        "            val_preds = self.forward(x_val)\n",
        "            val_acc=help.accuracy(val_preds,y_val)\n",
        "            val_accs.append(val_acc)\n",
        "            val_loss = help.compute_loss(y_val, val_preds,self.layers,self.loss_type,self.reg)\n",
        "\n",
        "            print(f'Train Loss:{train_loss} Train Acc: {train_acc} Val Acc: {val_acc} Val Loss: {val_loss}')  # printing the losses and accuracy after each epoch  \n",
        "            '''Wandb logging values of Train accuracy, Train loss, val accuracy and val loss'''\n",
        "            '''wandb.log(\n",
        "        {\"Train/Loss\": train_loss, \"Train/Accuracy\": train_acc, \"Val/Accuracy\": val_acc, \"Val/Loss\":val_loss,\"Epoch\":i})'''\n",
        "                \n",
        "          \n",
        "\n",
        "    '''\n",
        "    Method: Predict, model predictions on any data\n",
        "    Input: Test data to predict on\n",
        "    Output: predicted probabilities of the model on the test data.\n",
        "    '''\n",
        "    def predict(self,x):\n",
        "        preds=self.forward(x)\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main**"
      ],
      "metadata": {
        "id": "stbW9WUs-WIm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsb9vpdobdda"
      },
      "outputs": [],
      "source": [
        "'''Installing wandb and login'''\n",
        "! pip install wandb\n",
        "! wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDgNQQsvRU39"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Essential Imports including the dataset library.\n",
        "'''\n",
        "from keras.datasets import fashion_mnist # dataset to work on.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGxVbzO6Rk5t"
      },
      "outputs": [],
      "source": [
        "'''Datset loading'''\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcObKM89dOyY"
      },
      "source": [
        "**Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_type = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] "
      ],
      "metadata": {
        "id": "pYzUaGE0C1Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "generating a class list to pass as alist in the confucion matrix.\n",
        "'''\n",
        "class_list=[]\n",
        "for i in range(10):\n",
        "    for j in range(len(y_train)):\n",
        "        if y_train[j] == i :\n",
        "            class_list.append(class_type[y_train[j]])\n",
        "            break"
      ],
      "metadata": {
        "id": "WKtumgHACs_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQSTYYojclDT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "For plotting the images one for each class label\n",
        "'''\n",
        "images=[]\n",
        "labels=[]\n",
        "i=0\n",
        "while(len(labels)<10):\n",
        "  if y_train[i] not in labels:\n",
        "    labels.append(y_train[i])\n",
        "    images.append(x_train[i])\n",
        "    \n",
        "  i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2IpAiYZRmd8"
      },
      "outputs": [],
      "source": [
        "'''Date preprcessing'''\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "print(x_train.shape, x_test.shape)\n",
        "x_train = np.array(x_train/255., dtype=np.float32)\n",
        "x_test = np.array(x_test/255., dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvmHED2PRtEF"
      },
      "outputs": [],
      "source": [
        "'''Method to make labels into a onehot vector'''\n",
        "def one_hot(Y):\n",
        "    num_labels = len(set(Y))\n",
        "    new_Y = []\n",
        "    for label in Y:\n",
        "        encoding = np.zeros(num_labels)\n",
        "        encoding[label] = 1.\n",
        "        new_Y.append(encoding)\n",
        "    return np.array(new_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VVSVp5sR1t1"
      },
      "outputs": [],
      "source": [
        "'''Label conversion to onehot vectors'''\n",
        "y_train = one_hot(y_train)\n",
        "y_test = one_hot(y_test)\n",
        "y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KByWIJp49_oU"
      },
      "source": [
        "**Individual Checking for trying out the network implementation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8slgnxSMSEPv"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Individual testing hyper parameters\n",
        "'''\n",
        "epochs = 5\n",
        "lr = 1e-3\n",
        "batch_size = 16\n",
        "optimizer=\"Adam\"\n",
        "init_type=\"Xavier\"\n",
        "loss_type=\"CrossEntropy\"\n",
        "reg=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPcJt5BoSIZV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Building the model training\n",
        "'''\n",
        "model = Neural_Network()\n",
        "model.add(Layer(64, activation='tanh'))\n",
        "model.add(Layer(64, activation='tanh'))\n",
        "model.add(Layer(64, activation='tanh'))\n",
        "model.add(Layer(10, activation='softmax'))\n",
        "print(model.layers)\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs, learning_rate=lr, optimizer=optimizer,val_split=0.1,init_type=init_type,loss_type=loss_type,reg=reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf56YWhw-E_o"
      },
      "source": [
        "**Wandb Sweep for all hyper parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWy4xf2_1eU2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Hyper paramter sets for tuning the model on different combinations of the hyper parameters.\n",
        "Using the sweep functionality of Wandb \n",
        "'''\n",
        "sweep_config = {\n",
        "    'method': 'random', #bayes, random, grid methods can be used for tuning.\n",
        "    'metric': {\n",
        "      'name': 'Val/Accuracy', # goal is to maximize the validation accuracy.\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5,10]\n",
        "        },\n",
        "        'no_hidden_layer':{\n",
        "            'values': [3,4,5]  \n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3,1e-4]\n",
        "        },\n",
        "        'opt': {\n",
        "            'values': ['gd','sgdm','nesterov','rmsprop','adam','nadam']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu', 'sigmoid','tanh']\n",
        "        },\n",
        "        'batch_size':{\n",
        "            'values':[16,32,64]\n",
        "        },\n",
        "        'size_hidden':{\n",
        "            'values':[32,64,128]\n",
        "        },\n",
        "        'reg':{\n",
        "            'values': [0,0.0005,0.5]\n",
        "        },\n",
        "        'init_type':{\n",
        "            'values': ['Xavier','Random']  \n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBeQFrdp4YKm"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Project initialization in Wandb\n",
        "'''\n",
        "sweep_id = wandb.sweep(sweep_config, entity=entity, project=project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqyRu4n14rox"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Method: train for sweeping in wandb for hyper paramter tuning.\n",
        "'''\n",
        "def train():\n",
        "    steps = 0\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'epochs': 5,\n",
        "        'no_hidden_layer':3,\n",
        "        'learning_rate': 1e-3,\n",
        "        'opt':'adam',\n",
        "        'activation':'sigmoid',\n",
        "        'batch_size':16,\n",
        "        'size_hidden':32,\n",
        "        'reg':0,\n",
        "        'init_type':'Xavier'\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(project=project, entity=entity,config=config_defaults)\n",
        "    \n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    lr = config.learning_rate\n",
        "    epochs = config.epochs\n",
        "    opt = config.opt\n",
        "    acti=config.activation\n",
        "    batch_size = config.batch_size\n",
        "    hidden_size=config.size_hidden\n",
        "    reg=config.reg\n",
        "    init_type=config.init_type\n",
        "    no_hidden_layer=config.no_hidden_layer\n",
        "    if opt==\"gd\":\n",
        "        opt=\"GD\"\n",
        "    elif opt=='adam':\n",
        "      opt=\"Adam\"\n",
        "    elif opt=='rmsprop':\n",
        "      opt=\"RMSprop\"\n",
        "    elif opt=='sgdm':\n",
        "      opt='SGDM'\n",
        "    elif opt=='nadam':\n",
        "      opt=\"Nadam\"\n",
        "    elif opt=='nesterov':\n",
        "      opt=\"Nesterov\"\n",
        "    # Model training and sweeping the value\n",
        "    model = Neural_Network()\n",
        "    for i in range(no_hidden_layer):\n",
        "        model.add(Layer(hidden_size, activation=acti))\n",
        "\n",
        "    model.add(Layer(10, activation='softmax'))\n",
        "    print(model.layers)\n",
        "    model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs, learning_rate=lr, optimizer=opt,val_split=0.1,init_type=init_type,loss_type=\"SquaredError\",reg=reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzcrPafK6cqo"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train,count=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classes Plotting**"
      ],
      "metadata": {
        "id": "Q1ilY0pzCAPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Create your own project for plotting the classes or can use the same as before.\n",
        "'''\n",
        "entity=\"cs21m007_cs21m013\"\n",
        "project=\"Assignment_1_random_randomSeed_SE\""
      ],
      "metadata": {
        "id": "2mJERHP3zxXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "can Initialize a new project for plotting the classes.\n",
        "'''\n",
        "wandb.init(project=project, entity=entity)"
      ],
      "metadata": {
        "id": "ff0KfvB69863"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Code for plotting one image per each class, and visualizing it in Wandb media.\n",
        "'''\n",
        "num=10\n",
        "num_row = 2\n",
        "num_col = 5# plot images\n",
        "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
        "for i in range(num):\n",
        "    ax = axes[i//num_col, i%num_col]\n",
        "    ax.imshow(images[i], cmap='gray')\n",
        "    ax.set_title('Label: {}'.format(labels[i]))\n",
        "plt.tight_layout()\n",
        "wandb.log({'Clases':plt})"
      ],
      "metadata": {
        "id": "Wq_B2IZ1T8hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix and Testing**"
      ],
      "metadata": {
        "id": "cye2WZtZCE5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Can Initialize a new project or use the previous own for Tetsing and Confusion matrix visualization.\n",
        "'''\n",
        "wandb.init(project=project, entity=entity)"
      ],
      "metadata": {
        "id": "yIgGpUm_FdnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Best parameters obtained from Hyper parameter tuning using Wandb sweep is set as the below parameters.\n",
        "'''\n",
        "epochs = 10\n",
        "acti='tanh'\n",
        "lr = 1e-4\n",
        "batch_size = 64\n",
        "optimizer=\"Nadam\"\n",
        "init_type=\"Xavier\"\n",
        "loss_type=\"SquaredError\"\n",
        "reg=0\n",
        "hidden_size=128\n",
        "no_hidden_layer=4\n",
        "\n",
        "\n",
        "\n",
        "model = Neural_Network()\n",
        "\n",
        "for i in range(no_hidden_layer):\n",
        "        model.add(Layer(hidden_size, activation=acti))\n",
        "\n",
        "model.add(Layer(10, activation='softmax'))\n",
        "print(model.layers)\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs, learning_rate=lr, optimizer=optimizer,val_split=0.1,init_type=init_type,loss_type=loss_type,reg=reg)\n",
        "y_prob=model.predict(x_test)"
      ],
      "metadata": {
        "id": "_PongNAjSTJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help=Helper()\n",
        "accuracy=help.accuracy(y_test,y_prob)"
      ],
      "metadata": {
        "id": "IJDzDGC6FFoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "ZGxT8PUTGGdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "x_test = np.array(x_test/255., dtype=np.float32)"
      ],
      "metadata": {
        "id": "Cxm_VO08IXab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob=np.empty(np.shape(y_test))\n",
        "for i,x in enumerate(x_test):\n",
        "    y_prob[i]= (model.predict(x)[0]).argmax()"
      ],
      "metadata": {
        "id": "r_dbU0J4E1Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test,y_prob.shape"
      ],
      "metadata": {
        "id": "LE41ImmnF4zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Confusion Matrix logging.\n",
        "'''\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(preds=y_prob, y_true=y_test, class_names=class_list),\"Test Accuracy\": accuracy })"
      ],
      "metadata": {
        "id": "lSnwifPI-VGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Entropy Vs Squared Error"
      ],
      "metadata": {
        "id": "bFwb9emK_XGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Comparison of the Squared Error loss and Cross Entropy loss for best hyper paramter set.\n",
        "'''\n",
        "sweep_config = {\n",
        "    'method': 'grid', #grid will be enough as only 2 sweeps will be made.\n",
        "    'metric': {\n",
        "      'name': 'Val/Accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'no_hidden_layer':{\n",
        "            'values': [4]  \n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-4]\n",
        "        },\n",
        "        'opt': {\n",
        "            'values': ['nadam']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['tanh']\n",
        "        },\n",
        "        'batch_size':{\n",
        "            'values':[64]\n",
        "        },\n",
        "        'size_hidden':{\n",
        "            'values':[128]\n",
        "        },\n",
        "        'reg':{\n",
        "            'values': [0]\n",
        "        },\n",
        "        'init_type':{\n",
        "            'values': ['Xavier']  \n",
        "        },\n",
        "        'loss_type':{\n",
        "            'values': [\"SquaredError\",\"CrossEntropy\"]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "_CUp8Unu_ddz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=entity, project=project)"
      ],
      "metadata": {
        "id": "opZg-x8OAM75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Method: train for running the sweep in Wandb.\n",
        "'''\n",
        "def train():\n",
        "    steps = 0\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'epochs': 10,\n",
        "        'no_hidden_layer':4,\n",
        "        'learning_rate': 1e-4,\n",
        "        'opt':'nadam',\n",
        "        'activation':'tanh',\n",
        "        'batch_size':64,\n",
        "        'size_hidden':128,\n",
        "        'reg':0,\n",
        "        'init_type':'Xavier',\n",
        "        'loss_type':'CrossEntropy'\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(project=project, entity=entity,config=config_defaults)\n",
        "    \n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    lr = config.learning_rate\n",
        "    epochs = config.epochs\n",
        "    opt = config.opt\n",
        "    acti=config.activation\n",
        "    batch_size = config.batch_size\n",
        "    hidden_size=config.size_hidden\n",
        "    reg=config.reg\n",
        "    init_type=config.init_type\n",
        "    no_hidden_layer=config.no_hidden_layer\n",
        "    loss_type=config.loss_type\n",
        "    if opt==\"gd\":\n",
        "        opt=\"GD\"\n",
        "    elif opt=='adam':\n",
        "      opt=\"Adam\"\n",
        "    elif opt=='rmsprop':\n",
        "      opt=\"RMSprop\"\n",
        "    elif opt=='sgdm':\n",
        "      opt='SGDM'\n",
        "    elif opt=='nadam':\n",
        "      opt=\"Nadam\"\n",
        "    elif opt=='nesterov':\n",
        "      opt=\"Nesterov\"\n",
        "    # Model training here\n",
        "    model = Neural_Network()\n",
        "    for i in range(no_hidden_layer):\n",
        "        model.add(Layer(hidden_size, activation=acti))\n",
        "\n",
        "    model.add(Layer(10, activation='softmax'))\n",
        "    print(model.layers)\n",
        "    model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs, learning_rate=lr, optimizer=opt,val_split=0.1,init_type=init_type,loss_type=loss_type,reg=reg)"
      ],
      "metadata": {
        "id": "jAgqhwikAQGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Run the 2 sweeps.\n",
        "'''\n",
        "wandb.agent(sweep_id, train)"
      ],
      "metadata": {
        "id": "DsKjMN1iAxfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R83jFSAsA3jx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_1_FFN_fromScratch_without_outputs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}