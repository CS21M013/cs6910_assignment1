{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_FFN_Fashion_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Set the wandb credentials.\n",
        "'''\n",
        "project=\"Fashion_MNIST_best_parameter\"\n",
        "entity=\"cs21m007_cs21m013\""
      ],
      "metadata": {
        "id": "RtLAodzA1riN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_5CsOYjOYhO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Class: Layer\n",
        "Definations: Initialize_params, activation_fn\n",
        "'''\n",
        "\n",
        "import numpy as np  # numpy for implementing array operations inside the functions.\n",
        "class Layer:\n",
        "    '''\n",
        "    Method: __init__ constructor for base initializations\n",
        "    Input: no of hidden units of the layer, activation for the layer\n",
        "    Output: None\n",
        "    '''\n",
        "    def __init__(self, hidden_units: int, activation:str=None):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation = activation\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "    '''\n",
        "    Method: intialize_params for initializing the weights and biases of each layer\n",
        "    Input: dimension of the input to the layer, no of hidden neurons in the layer, the initialization type(Random or Xavier)\n",
        "    Output: None\n",
        "    ''' \n",
        "    def initialize_params(self, n_in, hidden_units,init_type):\n",
        "        np.random.seed(2)\n",
        "        if init_type==\"Random\":\n",
        "            self.W = 0.01*np.random.randn(n_in, hidden_units)\n",
        "            self.b = 0.01*np.random.randn(1,hidden_units)\n",
        "\n",
        "        elif init_type==\"Xavier\":\n",
        "            self.W = np.random.randn(n_in, hidden_units) * np.sqrt(2/n_in) \n",
        "            self.b = np.zeros((1, hidden_units))\n",
        "\n",
        "    '''\n",
        "    Method: activation_fn for defining the activation functions and thier derivatives to be used by the layers\n",
        "    Input: The computed pre activation of each layer\n",
        "    Output: calculates the activation value for forward prop or the derivative of the activation for backward prop\n",
        "    '''\n",
        "    def activation_fn(self, z, derivative=False):\n",
        "        '''\n",
        "        Relu activation and its derivative\n",
        "        '''\n",
        "        if self.activation == 'relu':\n",
        "            if derivative:\n",
        "                return np.where(z<=0,0,1)\n",
        "            return np.maximum(0, z)\n",
        "        '''\n",
        "        sigmoid activation and its derivative\n",
        "        '''\n",
        "        if self.activation == 'sigmoid':\n",
        "            if derivative:\n",
        "                return (1 / (1 + np.exp(-z))) * (1-(1 / (1 + np.exp(-z))))\n",
        "            return (1 / (1 + np.exp(-z)))\n",
        "        '''\n",
        "        tanh activation and its derivative\n",
        "        '''\n",
        "        if self.activation == 'tanh':\n",
        "            t=(np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
        "            if derivative:\n",
        "                return (1-t**2)\n",
        "            return t\n",
        "        '''\n",
        "        softmax function and its derivativ for the output layer with 10 neurons.\n",
        "        '''\n",
        "        if self.activation == 'softmax':\n",
        "            if derivative: \n",
        "                exp = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "                return exp / np.sum(exp, axis=0) * (1 - exp / np.sum(exp, axis=0))\n",
        "            exp = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "            return exp / np.sum(exp, axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Class: Helper\n",
        "Definations: Accruacy function, compute_loss function, create batches function.\n",
        "'''\n",
        "import numpy as np  # numpy for performing array operations.\n",
        "class Helper:\n",
        "    '''\n",
        "    Method: accuracy\n",
        "    Input: the true labels and the predicted proababilities generated by the model.\n",
        "    Output: Returns the accuracy of the model on the data.\n",
        "    '''\n",
        "    def accuracy(self,y,y_hat):\n",
        "        c = np.argmax(y_hat, axis=1) == np.argmax(y, axis=1)\n",
        "        acc = list(c).count(True) / len(c) * 100\n",
        "        return acc\n",
        "\n",
        "    '''\n",
        "    Method: compute_loss\n",
        "    Input: the true label, predicted probabilities, loss_type(SquarredError or CrossEntropy) and the regularization coefficient if any.\n",
        "    Output: Returns the value of the loss (SE or CE) plus the regularization loss if any.\n",
        "    '''\n",
        "    def compute_loss(self,Y, Y_hat,layers,loss_type=\"CrossEntropy\",reg=0):\n",
        "        if loss_type==\"CrossEntropy\":\n",
        "            m = Y.shape[0]\n",
        "            L = -1./m * np.sum(Y * np.log(Y_hat+0.0000000001))\n",
        "        elif loss_type==\"SquaredError\":\n",
        "            L = np.mean((Y- Y_hat)**2)\n",
        "\n",
        "        if reg!=0:\n",
        "            reg_error = 0.0                                                                       \n",
        "            for idx in layers.keys() :\n",
        "              reg_error += (reg/2)*(np.sum(np.square(layers[idx].W))) \n",
        "            L = L + reg_error\n",
        "\n",
        "        return L\n",
        "    \n",
        "    '''\n",
        "    Method: create_batches depending on the batch size for training\n",
        "    Input: the training data (X,y) and the batch size\n",
        "    Ouput: Batches of data for training based on the batch size.\n",
        "    '''\n",
        "    def create_batches(self,x, y, batch_size):\n",
        "        m = x.shape[0]\n",
        "        num_batches = m / batch_size\n",
        "        batches = []\n",
        "        for i in range(int(num_batches+1)):\n",
        "            batch_x = x[i*batch_size:(i+1)*batch_size]\n",
        "            batch_y = y[i*batch_size:(i+1)*batch_size]\n",
        "            batches.append((batch_x, batch_y))\n",
        "        \n",
        "        if m % batch_size == 0:\n",
        "            batches.pop(-1)\n",
        "\n",
        "        return batches\n",
        "    "
      ],
      "metadata": {
        "id": "8HJtRgK3O4Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Clss: Neural_Network\n",
        "Definitions: Constructor __init__, add , forward, backward, GDoptimize, \n",
        "              SGDMoptimize, Nesterovoptimize, RMSpropoptimize, Adamoptimize, \n",
        "              Nadamoptimize, fit, predict\n",
        "'''\n",
        "import numpy as np  # numpy to tackle all array related operations\n",
        "from sklearn.model_selection import train_test_split  # train test split for splitting the train data into further train and validation.\n",
        "class Neural_Network:\n",
        "    '''\n",
        "    Method: __init__ constructor for base initialization of layers, cache and gradients for each layer.\n",
        "    Input: None\n",
        "    Output: None\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.layers = dict()\n",
        "        self.cache = dict()\n",
        "        self.grads = dict()\n",
        "\n",
        "    '''\n",
        "    Method: add, to add the layer objects to the model (object of neural network).\n",
        "    Input: layer dictionary\n",
        "    Output: None\n",
        "    '''    \n",
        "    def add(self, layer):\n",
        "        self.layers[len(self.layers)+1] = layer\n",
        "\n",
        "    '''\n",
        "    Method: forward, for forward propagation of the model.\n",
        "    Input: input data and the initilization type of the W,b's of the layer\n",
        "    Output: Returns the predicted probability distribution after forward propagation\n",
        "    '''\n",
        "    def forward(self, x, init_type=\"Xavier\"):\n",
        "        for idx, layer in self.layers.items():\n",
        "\n",
        "            layer.input = np.array(x, copy=True)\n",
        "            if layer.W is None:\n",
        "                layer.initialize_params(layer.input.shape[-1], layer.hidden_units,init_type)  # initilaize the weights and the biases.\n",
        "\n",
        "            layer.Z = x @ layer.W + layer.b # linear pre activation\n",
        "        \n",
        "            if layer.activation is not None:\n",
        "                layer.A = layer.activation_fn(layer.Z) #applying non-linear activation function\n",
        "                x = layer.A\n",
        "            else:\n",
        "                x = layer.Z\n",
        "            self.cache[f'W{idx}'] = layer.W # storing the weights of the layer\n",
        "            self.cache[f'Z{idx}'] = layer.Z # storing the pre activation values of each layer\n",
        "            self.cache[f'A{idx}'] = layer.A # storing the activation values of each layer.\n",
        "        return x\n",
        "\n",
        "    '''\n",
        "    Method: backward, for backward propagation for generating the gradients for weight updation.\n",
        "    Input: true labels, loss_type, regularization coefficient\n",
        "    Output: None, but save the gradients in the grad dictionary of the model(Neural Network object)\n",
        "    '''\n",
        "    def backward(self, y, loss_type,reg=0):\n",
        "        last_layer_idx = max(self.layers.keys())\n",
        "        m = y.shape[0]\n",
        "        for idx in reversed(range(1, last_layer_idx+1)):  # move from output to inputs\n",
        "            if idx == last_layer_idx:\n",
        "                if loss_type==\"CrossEntropy\":\n",
        "                    self.grads[f'dZ{idx}'] = self.cache[f'A{idx}'] - y  # gradient wrt output layer for cross entropy loss\n",
        "                elif loss_type==\"SquaredError\":\n",
        "                    self.grads[f'dZ{idx}'] = (self.cache[f'A{idx}'] - y) * self.layers[idx].activation_fn(self.cache[f'Z{idx}'], derivative=True) # gradients wrt output layer for squared error loss\n",
        "            else:\n",
        "                self.grads[f'dZ{idx}'] = self.grads[f'dZ{idx+1}'] @ self.cache[f'W{idx+1}'].T *\\\n",
        "                                        self.layers[idx].activation_fn(self.cache[f'Z{idx}'], derivative=True) # gradients directly wrt to the pre-activation for each layer.\n",
        "\n",
        "\n",
        "            self.grads[f'dW{idx}'] = 1 / m * self.layers[idx].input.T @ self.grads[f'dZ{idx}'] + reg*self.layers[idx].W # gradients wrt the weights of each layer\n",
        "            self.grads[f'db{idx}'] = 1 / m * np.sum(self.grads[f'dZ{idx}'], axis=0, keepdims=True)  # gradients wrt the biases of each layer.\n",
        "            \n",
        "            assert self.grads[f'dW{idx}'].shape == self.cache[f'W{idx}'].shape\n",
        "\n",
        "    '''\n",
        "    Method: GDoptimize, basically the vanilla gradient descent\n",
        "    Input: learning_rate, idx indicating the layer index\n",
        "    Output: None, but performs the weight updations wrt the gradients.\n",
        "    '''\n",
        "    def GDoptimize(self, idx, learning_rate=1e-3):\n",
        "        \n",
        "        self.layers[idx].W -= learning_rate * self.grads[f'dW{idx}']  # W update\n",
        "        self.layers[idx].b -= learning_rate * self.grads[f'db{idx}']  # b update\n",
        "\n",
        "    '''\n",
        "    Method: SGDMoptimize, basically the momemtum based gradient descent.\n",
        "    Input: learning_rate, idx, mu - fixed momentum coefficient\n",
        "    Output: None, but weights, biases are updated\n",
        "    '''\n",
        "    def SGDMoptimize(self, idx, learning_rate=1e-3, mu=0.99):\n",
        "        m = dict()\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "\n",
        "        m[f'W{idx}'] = m[f'W{idx}'] * mu - learning_rate * self.grads[f'dW{idx}'] # momentum wrt W\n",
        "        m[f'b{idx}'] = m[f'b{idx}'] * mu - learning_rate * self.grads[f'db{idx}'] # momentum wrt b\n",
        "\n",
        "        self.layers[idx].W += m[f'W{idx}']  # W update\n",
        "        self.layers[idx].b += m[f'b{idx}']  # b update\n",
        "\n",
        "    '''\n",
        "    Method: Nesterovoptimize, nesterov accelerated gradien descent.\n",
        "    Input: learning rate, mu - fixed momentum coefficient, idx of the layer\n",
        "    Output: None, but updates the parameters(W,b)\n",
        "    '''\n",
        "    def Nesterovoptimize(self, idx, learning_rate=1e-3, mu=0.99):\n",
        "        m = dict()\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "\n",
        "        mW_prev =  np.array(m[f'W{idx}'], copy=True)\n",
        "        mb_prev = np.array(m[f'b{idx}'], copy=True)\n",
        "\n",
        "        m[f'W{idx}'] = m[f'W{idx}'] * mu - learning_rate * self.grads[f'dW{idx}'] # moemtum update wrt W\n",
        "        m[f'b{idx}'] = m[f'b{idx}'] * mu - learning_rate * self.grads[f'db{idx}'] # momentum update wrt b\n",
        "        # using the lookaheads\n",
        "        w_update = -mu * mW_prev + (1 + mu) * m[f'W{idx}'] \n",
        "        b_update = -mu * mb_prev + (1 + mu) * m[f'b{idx}']\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  # b update\n",
        "\n",
        "    '''\n",
        "    Method: RMSpropoptimize, basicall RMSprop gradient descent.\n",
        "    Input: idx of layer, learning rate, decay rate and epsilon\n",
        "    Output: None, updates the parameters.\n",
        "    '''\n",
        "    def RMSpropoptimize(self, idx, learning_rate=1e-3,decay_rate=0.99, epsilon=1e-8):\n",
        "        v = dict()\n",
        "        for i in self.layers.keys():\n",
        "            v[f'W{i}'] = 0\n",
        "            v[f'b{i}'] = 0\n",
        "        # using the learning rate decay\n",
        "        v[f'W{idx}'] = decay_rate * v[f'W{idx}'] + (1 - decay_rate) * self.grads[f'dW{idx}'] **2 \n",
        "        v[f'b{idx}'] = decay_rate * v[f'b{idx}'] + (1 - decay_rate) * self.grads[f'db{idx}'] **2\n",
        "        # update values calculation    \n",
        "        w_update = -learning_rate * self.grads[f'dW{idx}'] / (np.sqrt(v[f'W{idx}'] + epsilon))\n",
        "        b_update = -learning_rate * self.grads[f'db{idx}'] / (np.sqrt(v[f'b{idx}']+ epsilon))\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  # b update\n",
        "\n",
        "    '''\n",
        "    Method: Adamoptimize, Adam optimizer\n",
        "    Input: idx,steps,learing rate, beta1, beta2 and epsilon\n",
        "    Ouput: None, but updates the parameters\n",
        "    '''\n",
        "    def Adamoptimize(self, idx, steps, learning_rate=1e-3, beta1=0.99, beta2=0.999, epsilon=1e-8): \n",
        "        m = dict()\n",
        "        v = dict()\n",
        "\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "            v[f'W{i}'] = 0\n",
        "            v[f'b{i}'] = 0\n",
        "\n",
        "        dW = self.grads[f'dW{idx}']\n",
        "        db = self.grads[f'db{idx}']\n",
        "\n",
        "        # weights\n",
        "        m[f'W{idx}'] = beta1 * m[f'W{idx}'] + (1 - beta1) * dW\n",
        "        v[f'W{idx}'] = beta2 * v[f'W{idx}'] + (1 - beta2) * dW ** 2 \n",
        "        \n",
        "        # biases\n",
        "        m[f'b{idx}'] = beta1 * m[f'b{idx}'] + (1 - beta1) * db\n",
        "        v[f'b{idx}'] = beta2 * v[f'b{idx}'] + (1 - beta2) * db ** 2 \n",
        "\n",
        "        # take timestep into account for bias correction\n",
        "        mt_w  = m[f'W{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_w = v[f'W{idx}'] / (1 - beta2 ** steps)\n",
        "\n",
        "        mt_b  = m[f'b{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_b = v[f'b{idx}'] / (1 - beta2 ** steps)\n",
        "\n",
        "        w_update = - learning_rate * mt_w / (np.sqrt(vt_w) + epsilon)\n",
        "        b_update = - learning_rate * mt_b / (np.sqrt(vt_b) + epsilon)\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  # b update\n",
        "\n",
        "    '''\n",
        "    Method: Nadamoptimize, nesterov accelerated Adam\n",
        "    Input: idx of layer, steps, learing rate, beat1, beta2, epsilon\n",
        "    Output: None, but updates the parameters.\n",
        "    '''\n",
        "    def Nadamoptimize(self, idx, steps,learning_rate=1e-3, beta1=0.99, beta2=0.999, epsilon=1e-8): \n",
        "        m = dict()\n",
        "        v = dict()\n",
        "\n",
        "        for i in self.layers.keys():\n",
        "            m[f'W{i}'] = 0\n",
        "            m[f'b{i}'] = 0\n",
        "            v[f'W{i}'] = 0\n",
        "            v[f'b{i}'] = 0\n",
        "        dW = self.grads[f'dW{idx}']\n",
        "        db = self.grads[f'db{idx}']\n",
        "        # weights\n",
        "        m[f'W{idx}'] = beta1 * m[f'W{idx}'] + (1 - beta1) * dW\n",
        "        v[f'W{idx}'] = beta2 * v[f'W{idx}'] + (1 - beta2) * dW ** 2 \n",
        "            \n",
        "        # biases\n",
        "        m[f'b{idx}'] = beta1 * m[f'b{idx}'] + (1 - beta1) * db\n",
        "        v[f'b{idx}'] = beta2 * v[f'b{idx}'] + (1 - beta2) * db ** 2 \n",
        "\n",
        "        # take timestep into account for bias correction\n",
        "        mt_w  = m[f'W{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_w = v[f'W{idx}'] / (1 - beta2 ** steps)\n",
        "\n",
        "        mt_b  = m[f'b{idx}'] / (1 - beta1 ** steps) #accumulated history\n",
        "        vt_b = v[f'b{idx}'] / (1 - beta2 ** steps)\n",
        "        # accelerated momentum incorporation into adam\n",
        "        w_update = - learning_rate / (np.sqrt(vt_w) + epsilon) * (beta1 * mt_w + (1 - beta1) *  dW / (1 - beta1 ** steps))\n",
        "        b_update = - learning_rate / (np.sqrt(vt_b) + epsilon) * (beta1 * mt_b + (1 - beta1) *  db / (1 - beta1 ** steps))\n",
        "\n",
        "        self.layers[idx].W += w_update  # W update\n",
        "        self.layers[idx].b += b_update  #b update\n",
        "            \n",
        "    '''\n",
        "    Method: fit, used to train the model by combining forward_prop, back_prop and gradient descent weight updation.\n",
        "    Input: Training data, batch_size, epochs, learning rate, optimizer to use, val_split factor, initialization type of the weights and biases, loss type, and the regularization coefficient\n",
        "    Output: None, but performs the training of the model \n",
        "    '''\n",
        "    def fit(self, x_train, y_train,batch_size=32,epochs=500, learning_rate=1e-3, optimizer=\"GD\",val_split=0.1,init_type=\"Xavier\",loss_type=\"CrossEntropy\",reg=0):\n",
        "        train_accs = [] #stores the training accuracy for each epoch\n",
        "        val_accs = [] #stores the validation accuracy after each epoch\n",
        "        help=Helper() #creating a object of the Helper class for helper functions\n",
        "        \n",
        "        '''Initializations'''\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.init_type=init_type\n",
        "        self.reg=reg\n",
        "        self.loss_type=loss_type\n",
        "\n",
        "        '''Splitting the training data into train and val data based on the val_split value''' \n",
        "        x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=val_split,stratify=y_train,random_state=42)\n",
        "\n",
        "        '''Training Cycle'''\n",
        "        for i in range(1, self.epochs+1):\n",
        "            print(f'Epoch {i}')\n",
        "            batches = help.create_batches(x_train, y_train, batch_size) # create batches based on the batch size\n",
        "            epoch_loss = []\n",
        "            steps = 0 #count the steps in each epoch\n",
        "            \n",
        "            for x, y in batches:\n",
        "                steps += 1\n",
        "                '''Forward Propagation'''\n",
        "                preds = self.forward(x,self.init_type)\n",
        "   \n",
        "                '''backward propagation'''\n",
        "                self.backward(y,self.loss_type,self.reg)\n",
        "                \n",
        "                '''update weights and biases of each layer using the corresponding optimizer'''\n",
        "                for idx in self.layers.keys():\n",
        "                    if self.optimizer ==\"GD\":\n",
        "                        self.GDoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"SGDM\":\n",
        "                        self.SGDMoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"Nesterov\":\n",
        "                        self.Nesterovoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"RMSprop\":\n",
        "                        self.RMSpropoptimize(idx, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"Adam\":\n",
        "                        self.Adamoptimize(idx, steps, learning_rate=self.learning_rate)\n",
        "                    elif self.optimizer==\"Nadam\":\n",
        "                        self.Nadamoptimize(idx, steps, learning_rate=self.learning_rate)\n",
        "                \n",
        "            '''Predict with network on x_train'''\n",
        "            train_preds = self.forward(x_train)\n",
        "            train_loss = help.compute_loss(y, preds,self.layers,self.loss_type,self.reg)\n",
        "            train_acc=help.accuracy(train_preds,y_train)\n",
        "            train_accs.append(train_acc)\n",
        "            \n",
        "            '''predcit with network on validation data'''\n",
        "            val_preds = self.forward(x_val)\n",
        "            val_acc=help.accuracy(val_preds,y_val)\n",
        "            val_accs.append(val_acc)\n",
        "            val_loss = help.compute_loss(y_val, val_preds,self.layers,self.loss_type,self.reg)\n",
        "\n",
        "            print(f'Train Loss:{train_loss} Train Acc: {train_acc} Val Acc: {val_acc} Val Loss: {val_loss}')  # printing the losses and accuracy after each epoch  \n",
        "            '''Wandb logging values of Train accuracy, Train loss, val accuracy and val loss'''\n",
        "            wandb.log(\n",
        "        {\"Train/Loss\": train_loss, \"Train/Accuracy\": train_acc, \"Val/Accuracy\": val_acc, \"Val/Loss\":val_loss,\"Epoch\":i})\n",
        "                     \n",
        "\n",
        "    '''\n",
        "    Method: Predict, model predictions on any data\n",
        "    Input: Test data to predict on\n",
        "    Output: predicted probabilities of the model on the test data.\n",
        "    '''\n",
        "    def predict(self,x):\n",
        "        preds=self.forward(x)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "4eYJnhuUO6kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wandb credentials and login"
      ],
      "metadata": {
        "id": "HLgny_PqUT_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Installing wandb and login\n",
        "'''\n",
        "! pip install wandb\n",
        "! wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R6ZQJfoO7KV",
        "outputId": "c65906e0-a183-4e1f-a63c-7c2441661e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 133 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 174 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 194 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 204 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 215 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 225 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 235 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 245 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 256 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 266 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 276 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 286 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 296 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 307 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 317 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 327 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 337 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 348 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 358 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 368 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 378 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 389 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 399 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 409 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 419 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 430 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 440 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 450 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 460 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 471 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 481 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 491 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 501 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 512 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 522 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 532 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 542 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 552 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 563 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 573 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 583 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 593 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 604 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 614 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 624 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 634 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 645 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 655 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 665 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 675 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 686 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 696 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 706 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 716 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 727 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 737 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 747 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 757 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 768 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 778 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 788 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 798 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 808 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 819 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 829 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 839 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 849 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 860 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 870 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 880 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 890 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 901 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 911 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 921 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 931 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 942 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 952 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 962 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 972 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 983 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 993 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.0 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.0 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.2 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.2 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.2 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.3 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.4 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.5 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.5 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.5 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.6 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.7 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.7 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.7 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.7 MB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 10.2 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.6-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=eb07b3bef98c0498206c9c8a01f3c99bc6b8c8fbfc5b575d59786f794054b933\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.6 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.10 yaspin-2.1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Essential Imports including the dataset library.\n",
        "'''\n",
        "from keras.datasets import fashion_mnist # dataset to work on.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb"
      ],
      "metadata": {
        "id": "JmtZEWSOO8sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Datset loading'''\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "pjsA7fULPHoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Date preprcessing'''\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "print(x_train.shape, x_test.shape)\n",
        "x_train = np.array(x_train/255., dtype=np.float32)\n",
        "x_test = np.array(x_test/255., dtype=np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPgiqanYPKe-",
        "outputId": "01800c8d-5e8b-4edf-fdd3-9c8e02243a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784) (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Method to make labels into a onehot vector'''\n",
        "def one_hot(Y):\n",
        "    num_labels = len(set(Y))\n",
        "    new_Y = []\n",
        "    for label in Y:\n",
        "        encoding = np.zeros(num_labels)\n",
        "        encoding[label] = 1.\n",
        "        new_Y.append(encoding)\n",
        "    return np.array(new_Y)"
      ],
      "metadata": {
        "id": "JJw-2Xp1PNVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Label conversion to onehot vectors'''\n",
        "y_train = one_hot(y_train)\n",
        "y_test = one_hot(y_test)\n",
        "y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDHoHKJfPOnU",
        "outputId": "2ea8bfee-df6a-4ede-c804-41d32c9c6d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (10000, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Method: train, for sweep in wandb for hyper parameter tuning\n",
        "'''\n",
        "def train():\n",
        "    steps = 0\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'epochs': 10,\n",
        "        'no_hidden_layer':4,\n",
        "        'learning_rate': 1e-3,\n",
        "        'opt':'adam',\n",
        "        'activation':'tanh',\n",
        "        'batch_size':64,\n",
        "        'size_hidden':128,\n",
        "        'reg':0,\n",
        "        'init_type':'Xavier'\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(project=project, entity=entity,config=config_defaults)\n",
        "    \n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    lr = config.learning_rate\n",
        "    epochs = config.epochs\n",
        "    opt = config.opt\n",
        "    acti=config.activation\n",
        "    batch_size = config.batch_size\n",
        "    hidden_size=config.size_hidden\n",
        "    reg=config.reg\n",
        "    init_type=config.init_type\n",
        "    no_hidden_layer=config.no_hidden_layer\n",
        "    if opt==\"gd\":\n",
        "        opt=\"GD\"\n",
        "    elif opt=='adam':\n",
        "      opt=\"Adam\"\n",
        "    elif opt=='rmsprop':\n",
        "      opt=\"RMSprop\"\n",
        "    elif opt=='sgdm':\n",
        "      opt='SGDM'\n",
        "    elif opt=='nadam':\n",
        "      opt=\"Nadam\"\n",
        "    elif opt=='nesterov':\n",
        "      opt=\"Nesterov\"\n",
        "    # Model training here and sweeping the values.\n",
        "    model = Neural_Network()\n",
        "    for i in range(no_hidden_layer):\n",
        "        model.add(Layer(hidden_size, activation=acti))\n",
        "\n",
        "    model.add(Layer(10, activation='softmax'))\n",
        "    print(model.layers)\n",
        "    model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs, learning_rate=lr, optimizer=opt,val_split=0.1,init_type=init_type,loss_type=\"CrossEntropy\",reg=reg)"
      ],
      "metadata": {
        "id": "SOMRUNDNPWjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing on the Best parameter set and generaing the Confusion Matrix**"
      ],
      "metadata": {
        "id": "k4NVrYjQp6j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Project initialization\n",
        "'''\n",
        "wandb.init(project=project, entity=entity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "ZXgZEqJKp8vi",
        "outputId": "e8c9bf8c-2f25-4de7-8aec-d9fbde6eb992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/cs21m007_cs21m013/Fashion_MNIST_best_parameter/runs/oeyunf2h\" target=\"_blank\">astral-sweep-1</a></strong> to <a href=\"https://wandb.ai/cs21m007_cs21m013/Fashion_MNIST_best_parameter\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/cs21m007_cs21m013/Fashion_MNIST_best_parameter/sweeps/iaaqb0vu\" target=\"_blank\">https://wandb.ai/cs21m007_cs21m013/Fashion_MNIST_best_parameter/sweeps/iaaqb0vu</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f0868438750>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/cs21m007_cs21m013/Fashion_MNIST_best_parameter/runs/oeyunf2h?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Testing the models on Fashion_MNIST data for the best configuration.\n",
        "'''\n",
        "'''\n",
        "Best Hyper parameter set\n",
        "'''\n",
        "epochs = 10\n",
        "acti='tanh'\n",
        "lr = 1e-4\n",
        "batch_size = 64\n",
        "optimizer=\"RMSprop\"\n",
        "init_type=\"Xavier\"\n",
        "loss_type=\"CrossEntropy\"\n",
        "reg=0.0005\n",
        "hidden_size=64\n",
        "no_hidden_layer=4\n",
        "\n",
        "\n",
        "\n",
        "model = Neural_Network()\n",
        "\n",
        "for i in range(no_hidden_layer):\n",
        "        model.add(Layer(hidden_size, activation=acti))\n",
        "\n",
        "model.add(Layer(10, activation='softmax'))\n",
        "print(model.layers)\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs, learning_rate=lr, optimizer=optimizer,val_split=0.1,init_type=init_type,loss_type=loss_type,reg=reg)\n",
        "y_prob=model.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2Ub_KVGqEfS",
        "outputId": "496a8c0e-3f33-42d5-f288-0827d057c6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: <__main__.Layer object at 0x7f086845bbd0>, 2: <__main__.Layer object at 0x7f086845bc90>, 3: <__main__.Layer object at 0x7f086845bcd0>, 4: <__main__.Layer object at 0x7f086845bd10>, 5: <__main__.Layer object at 0x7f086845bc50>}\n",
            "Epoch 1\n",
            "Train Loss:0.5043917651151516 Train Acc: 83.8537037037037 Val Acc: 83.28333333333333 Val Loss: 0.5913077578863284\n",
            "Epoch 2\n",
            "Train Loss:0.498067102118496 Train Acc: 85.65 Val Acc: 84.8 Val Loss: 0.5479379289976873\n",
            "Epoch 3\n",
            "Train Loss:0.48675478880167533 Train Acc: 86.52222222222223 Val Acc: 85.5 Val Loss: 0.5174020936693813\n",
            "Epoch 4\n",
            "Train Loss:0.46506470564577196 Train Acc: 87.62037037037037 Val Acc: 86.55000000000001 Val Loss: 0.4951242561134436\n",
            "Epoch 5\n",
            "Train Loss:0.4672634232792543 Train Acc: 87.97962962962963 Val Acc: 87.01666666666667 Val Loss: 0.48363019308616223\n",
            "Epoch 6\n",
            "Train Loss:0.4499980528204986 Train Acc: 88.1962962962963 Val Acc: 87.55 Val Loss: 0.47249849818380407\n",
            "Epoch 7\n",
            "Train Loss:0.429947433281647 Train Acc: 87.93148148148148 Val Acc: 87.16666666666667 Val Loss: 0.4792077451932598\n",
            "Epoch 8\n",
            "Train Loss:0.44482144478312524 Train Acc: 88.09074074074074 Val Acc: 87.18333333333334 Val Loss: 0.48025798420718896\n",
            "Epoch 9\n",
            "Train Loss:0.45973572178040695 Train Acc: 88.22777777777779 Val Acc: 87.4 Val Loss: 0.4817485880732244\n",
            "Epoch 10\n",
            "Train Loss:0.45671812083652213 Train Acc: 88.43333333333334 Val Acc: 87.51666666666667 Val Loss: 0.47964361374157716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help=Helper()\n",
        "accuracy=help.accuracy(y_test,y_prob)"
      ],
      "metadata": {
        "id": "x6w1NBImqGMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "UVEtPjAkqH2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_type = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] "
      ],
      "metadata": {
        "id": "l4owaMMisWv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_list=[]\n",
        "for i in range(10):\n",
        "    for j in range(len(y_train)):\n",
        "        if y_train[j] == i :\n",
        "            class_list.append(class_type[y_train[j]])\n",
        "            break"
      ],
      "metadata": {
        "id": "su8727oVsXWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "x_test = np.array(x_test/255., dtype=np.float32)"
      ],
      "metadata": {
        "id": "NGtcKZ2kqJAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob=np.empty(np.shape(y_test))\n",
        "for i,x in enumerate(x_test):\n",
        "    y_prob[i]= (model.predict(x)[0]).argmax()"
      ],
      "metadata": {
        "id": "pfxQmfKcqLBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test,y_prob.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALq2xqT3qMfS",
        "outputId": "d0c7df50-58b2-4ab1-cdbe-c4e1fb9746cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([9, 2, 1, ..., 8, 1, 5], dtype=uint8), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "confusion amtrix logging\n",
        "'''\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(preds=y_prob, y_true=y_test, class_names=class_list),\"Test Accuracy\": accuracy })"
      ],
      "metadata": {
        "id": "UbapwbCnqOMx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}